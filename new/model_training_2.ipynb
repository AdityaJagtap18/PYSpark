{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885535c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read chunk 1\n",
      "Read chunk 2\n",
      "Read chunk 3\n",
      "Read chunk 4\n",
      "Read chunk 5\n",
      "Read chunk 6\n",
      "Read chunk 7\n",
      "Read chunk 8\n",
      "Read chunk 9\n",
      "Read chunk 10\n",
      "Read chunk 11\n",
      "Read chunk 12\n",
      "Read chunk 13\n",
      "Read chunk 14\n",
      "Read chunk 15\n",
      "Read chunk 16\n",
      "Read chunk 17\n",
      "Read chunk 18\n",
      "Read chunk 19\n",
      "Read chunk 20\n",
      "Read chunk 21\n",
      "Read chunk 22\n",
      "Read chunk 23\n",
      "Read chunk 24\n",
      "Read chunk 25\n",
      "Read chunk 26\n",
      "Read chunk 27\n",
      "Read chunk 28\n",
      "Read chunk 29\n",
      "Read chunk 30\n",
      "Read chunk 31\n",
      "Read chunk 32\n",
      "Saved: data/hnm/processed/transactions_sample.csv shape: (1788324, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "src = \"data/hnm/transactions_train.csv\"\n",
    "out = \"data/hnm/processed/transactions_sample.csv\"\n",
    "os.makedirs(\"data/hnm/processed\", exist_ok=True)\n",
    "\n",
    "# Take last ~2M rows by reading in chunks and keeping recent chunks\n",
    "chunksize = 1_000_000\n",
    "keep_chunks = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(src, chunksize=chunksize, dtype={\"customer_id\": str, \"article_id\": str})):\n",
    "    keep_chunks.append(chunk)\n",
    "    # keep only last 2 chunks (~2M rows) in memory\n",
    "    if len(keep_chunks) > 2:\n",
    "        keep_chunks.pop(0)\n",
    "    print(f\"Read chunk {i+1}\")\n",
    "\n",
    "sample = pd.concat(keep_chunks, ignore_index=True)\n",
    "sample.to_csv(out, index=False)\n",
    "print(\"Saved:\", out, \"shape:\", sample.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fed5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea4725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 150000\n"
     ]
    }
   ],
   "source": [
    "items = pd.read_csv(\"data/hnm/processed/articles_features.csv\", dtype={\"article_id\": str})\n",
    "users = pd.read_csv(\"data/hnm/processed/customers_features.csv\", dtype={\"customer_id\": str})\n",
    "tx    = pd.read_csv(\"data/hnm/processed/transactions_sample.csv\",\n",
    "                    dtype={\"customer_id\": str, \"article_id\": str})\n",
    "\n",
    "tx = tx.drop_duplicates([\"customer_id\",\"article_id\"])\n",
    "\n",
    "# 8GB: start 150k; 16GB: start 300k–500k\n",
    "POS_TARGET = 150_000\n",
    "tx = tx.sample(POS_TARGET, random_state=42)\n",
    "\n",
    "tx[\"label\"] = 1\n",
    "print(\"Positives:\", len(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31775c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "all_articles = items[\"article_id\"].dropna().unique()\n",
    "\n",
    "NEG_PER_POS = 1\n",
    "MAX_NEG_PER_CUST = 20   # start 20; if stable you can raise to 30+\n",
    "\n",
    "cust_pos = tx.groupby(\"customer_id\")[\"article_id\"].apply(lambda s: s.unique())\n",
    "\n",
    "neg_rows = []\n",
    "for cust, bought_arr in cust_pos.items():\n",
    "    n_pos = len(bought_arr)\n",
    "    n_neg = min(n_pos * NEG_PER_POS, MAX_NEG_PER_CUST)\n",
    "\n",
    "    pool = np.setdiff1d(all_articles, bought_arr)\n",
    "    if len(pool) == 0 or n_neg == 0:\n",
    "        continue\n",
    "\n",
    "    n_neg = min(n_neg, len(pool))\n",
    "    sampled = rng.choice(pool, size=n_neg, replace=False)\n",
    "\n",
    "    neg_rows.extend([(cust, a, 0) for a in sampled])\n",
    "\n",
    "neg = pd.DataFrame(neg_rows, columns=[\"customer_id\",\"article_id\",\"label\"])\n",
    "data = pd.concat([tx[[\"customer_id\",\"article_id\",\"label\"]], neg], ignore_index=True)\n",
    "\n",
    "print(\"Pos:\", len(tx), \"Neg:\", len(neg), \"Total:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a575f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(items, on=\"article_id\", how=\"left\")\n",
    "data = data.merge(users, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    data, test_size=0.2, random_state=42, stratify=data[\"label\"]\n",
    ")\n",
    "\n",
    "X_train = train_df.drop(columns=[\"label\"])\n",
    "y_train = train_df[\"label\"].astype(int)\n",
    "X_test  = test_df.drop(columns=[\"label\"])\n",
    "y_test  = test_df[\"label\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319642e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(model_name, model, pkl_name):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"prep\", preprocess),\n",
    "        (\"clf\", model)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    pr  = average_precision_score(y_test, proba)\n",
    "\n",
    "    print(f\"{model_name} AUC    :\", round(auc, 4))\n",
    "    print(f\"{model_name} PR-AUC :\", round(pr, 4))\n",
    "\n",
    "    os.makedirs(\"data/hnm/models\", exist_ok=True)\n",
    "    with open(f\"data/hnm/models/{pkl_name}\", \"wb\") as f:\n",
    "        pickle.dump(pipe, f)\n",
    "\n",
    "    print(\"Saved →\", pkl_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bb4d4",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_model(\n",
    "    \"Logistic Regression\",\n",
    "    LogisticRegression(max_iter=300),\n",
    "    \"logreg.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3109f0",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344bf7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_model(\n",
    "    \"Random Forest\",\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"rf.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116be2d",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5defafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_model(\n",
    "    \"Gradient Boosting\",\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    \"gbdt.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350e608",
   "metadata": {},
   "source": [
    "AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "train_one_model(\n",
    "    \"AdaBoost\",\n",
    "    AdaBoostClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"adaboost.pkl\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
